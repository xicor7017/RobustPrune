Add readme for training and pruning overfited models

Trained with AdamW, prunned with:
    AdamW       Works
    AdamW       Works
    SGD         Does not work

ReLU (AdamW) works
ReLU (Adam)  works
ReLU (SGD)   WORKS !!  (More iterations)

Just 2 instead of 10 weight drops
ReLU (SGD)  works!
ReLU (AdamW) works

Just 2 instead of 10 weight drops
Tanh (AdamW) works
Tanh (SGD) Does not work

---------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------
Conclusions:
    Synthetic testing is fairly robust to ReLU vs Tanh
    SGD can work but also doesnt sometimes, may require tuning of how many neurons to prune.
    
    New testing: 
        Seems to work with bias as well.
        Works without freezing pruned gradients as well, however, no zero weights in the network in that case.
---------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------

Biased dataset (In complex situations) can be seen as distributional shift.


____________________________________

ToDo:
    - Package nicely and push: Separate arguments for transformer as MLP.
    - Try with the common hook that GPT suggested.

    Then 2 parallel things
    1. Find the insight on deciding num_prune / amount training.
    2. Try all architectures with common hooks.