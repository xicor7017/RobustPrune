1. freeze_pruned_gradients Doesnt seem to be important.
2. force_zero_weights is detrimental

Optim testing:
    AdamW       Works
    Adam        Works
    SGD         Almost works with more training / less pruning.

Activations testing:
    ReLU (AdamW)    Works
    ReLU (Adam)     Assuming that it works
    ReLU (SGD)      Works with more training / less pruning.

ToDo:
    - Test current pruning is working (Just run the bash script)
    - The current Prune_utils.py only support nn.Linear. Extend it to our use cases. Can we make it more general to directly nn.Parameters() ?
    - Make datasets for images, sequences, transformers and test the common prune_utils.py on them.
    - Find the insight on deciding num_prune / amount training.
    - Test the insight.
    - Start doing the paper experiments.

The challenge is that we can enforce zero weights, but then the model "looses" capacity / plasticity to learn new stuff:
THINK OF A RATIO, THE WEIGHTS THAT WE ARE ZEROING RATIO WITH THE WEIGHTS THAT WE ARE ENFORCING TO BE ZERO.

WE DO NOT WANT PRUNING FOR FASTER INFERENCE. SO WE DO NOT CARE FOR THIS : If we dont enforce zeros weights, then the model doesnt prune a significant number of weights.

https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html

https://arxiv.org/pdf/2506.11615


__________________________________
1. candidate scoring should not score std zero candidates.
2. global count of absolute zero indices.